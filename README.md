# SimpleGradientDescent
A simple example of Gradient Descent algorithms.(Homework of the large-scale optimization course.)
I compared different stepsize selection methods, which are fixed stepsize, Armijo-Goldstein rule and Wolfe-Powell rule, in this simple Gradient Descent algorithm.
My goal is to minimize a quadratic function $$f(\mathbf{x})=\frac{1}{2}\mathbf{x}^H\mathbf{A}\mathbf{x}$$, where $\mathbf{A}$ is randomly generated and positive semidefinite.

Some simulation results are placed here

<div align=center>
<img src="https://github.com/LiZhuoRan0/SimpleGradientDescent/blob/main/SomeOfSimulations/N_10.jpg"/>
</div>

<div align=center>
<img src="https://github.com/LiZhuoRan0/SimpleGradientDescent/blob/main/SomeOfSimulations/N_100.jpg"/>
</div>
